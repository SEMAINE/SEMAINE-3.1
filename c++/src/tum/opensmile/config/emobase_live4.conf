
///////////////////////////////////////////////////////////////////////////////////////
///////// > openSMILE configuration file for live emotion recognition < ///////////////
/////////   base set of 988 features, 1st level functionals          //////////////////
/////////   of low-level descriptors such as MFCC, Pitch, LSP, ...   //////////////////
/////////                                                            //////////////////
/////////  * written 2009 by Florian Eyben *                         //////////////////
/////////                                                            //////////////////
///////// (c) 2009 by Florian Eyben, Martin Wöllmer, Björn Schuller  //////////////////
/////////     see the file COPYING for details                       //////////////////
///////////////////////////////////////////////////////////////////////////////////////


///////////////////////////////////////////////////////////////////////////////////////
;
; This section is always required in openSMILE configuration files
;   it configures the componentManager and gives a list of all components which are to be loaded
; The order in which the components are listed should match 
;   the order of the data flow for most efficient processing
;
///////////////////////////////////////////////////////////////////////////////////////
[componentInstances:cComponentManager]
 ;this line configures the default data memory:
instance[dataMemory].type=cDataMemory
 ;;; PortAudio live sound card input
instance[waveIn].type=cPortaudioSource
instance[fr25].type=cFramer
 ;; VAD (energy-based)
instance[energy].type=cEnergy
instance[turn].type=cTurnDetector
 ;;; 40 ms frames features:
instance[fr40].type=cFramer
instance[w40].type=cWindower
instance[fft40].type=cTransformFFT
instance[fftmagphase40].type=cFFTmagphase
instance[acf40].type=cAcf
instance[cepstrum40].type=cAcf
 ; Pitch...
instance[pitchACF].type=cPitchACF
 ;;; 25 ms frames features:
instance[pe].type=cVectorPreemphasis
instance[win].type=cWindower
instance[fft].type=cTransformFFT
instance[fftmagphase].type=cFFTmagphase
instance[mspec].type=cMelspec
 ; MFCC
instance[mfcc].type=cMfcc
instance[lpc].type=cLpc
 ; Line Spectral Frequencies
instance[lsp].type=cLsp
 ; Zero-Crossings
instance[mzcr].type=cMZcr
 ; Intensity and Loudness (narrow-band approximation)
instance[intens].type=cIntensity
 ;;; all LLD concattenated and smoothed using a moving average filter
instance[lld].type=cContourSmoother
 ; delta coefficients of LLD
instance[delta1].type=cDeltaRegression
 ;;; functionals over FULL input (e.g. turns)
instance[functL1].type=cFunctionals
 ;;; write instances to a Weka ARFF file (comment out to disable)
instance[arffsink].type=cArffSink
 ;;; save recorded and segmented turn to a wave file (comment out to disable)
instance[turnDump].type=cWaveSinkCut
 ;;; live classification of emotion (comment out lines to disable them):
instance[arousal].type=cLibsvmLiveSink
instance[valence].type=cLibsvmLiveSink
instance[emodbEmotion].type=cLibsvmLiveSink
instance[abcAffect].type=cLibsvmLiveSink
instance[avicInterest].type=cLibsvmLiveSink

;; run single threaded (nThreads=1)
; NOTE: a single thread is more efficient for processing small files, since multi-threaded processing involves more 
;       overhead during startup, which will make the system slower in the end
nThreads=1
;; do not show any internal dataMemory level settings 
; (if you want to see them set the value to 1, 2, 3, or 4, depending on the amount of detail you wish)
printLevelStats=0


/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////   component configuration  ////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
; the following sections configure the components listed above
; a help on configuration parameters can be obtained with 
;  SMILExtract -H
; or
;  SMILExtract -H configTypeName (= componentTypeName)
/////////////////////////////////////////////////////////////////////////////////////////////

[waveIn:cPortaudioSource]
 ; this sets the level this component writes to
 ; the level will be created by this component
 ; no other components may write to a level having the same name
writer.dmLevel=wave
 ; this sets the portaudio recording device, leave commented out to use the default device
device = \cm[device(D){-1}:portaudio device number to record from]
 ; uncomment the following line to obtain a list of available devices when you call
 ;   SMILExtract -C config/emobase_live4.conf
 ; after displaying the device list, SMILExtract will exit. 
 ; Thus, for normal operation you have to comment the line out again
;listDevices=1
 ; record in mono
channels = 1
 ; we want to record audio in 16kHz from the audio device
 ; Note that some audio devices do not support this sample rate,
 ; if this is the case, try a different device, or use a resampling component
 ; (which is not yet integrated in a working version...)
sampleRate = 16000
 ; size of chunks returned by PortAudio (the latency is >= this value)
audioBuffersize_sec = 0.16
 ; the buffersize of the pcm data ringbuffer (don't make this too small, or you might loose data, 
 ;  if your system load is low to normal, this size also does not affect the latency. 
 ;  Only during high load this buffersize is an upper limit for the latency.)
buffersize_sec=2.0
listDevices=\cm[devlist{0}:list devices]

[fr40:cFramer]
reader.dmLevel=wave
writer.dmLevel=frames40
frameSize = 0.040
frameStep = 0.010

[w40:cWindower]
reader.dmLevel=frames40
writer.dmLevel=win40frame
winFunc = ham
gain = 1.0


[fft40:cTransformFFT]
reader.dmLevel=win40frame
writer.dmLevel=fftc40

[fftmagphase40:cFFTmagphase]
reader.dmLevel=fftc40
writer.dmLevel=fftmag40


[acf40:cAcf]
reader.dmLevel=fftmag40
writer.dmLevel=acf40

[cepstrum40:cAcf]
reader.dmLevel=fftmag40
writer.dmLevel=cepstrum40
cepstrum=1

[pitchACF:cPitchACF]
  ; the pitchACF component must ALWAYS read from acf AND cepstrum in the given order!
reader.dmLevel=acf40;cepstrum40
writer.dmLevel=pitch
processArrayFields=0
F0=1
voiceProb=1
HNR=0
F0env=1
voiceQual=0

[fr25:cFramer]
reader.dmLevel=wave
writer.dmLevel=frames
frameSize = 0.025
frameStep = 0.010


[pe:cVectorPreemphasis]
reader.dmLevel=frames
writer.dmLevel=framespe
k=0.97

[win:cWindower]
reader.dmLevel=framespe
writer.dmLevel=winframe
winFunc = ham
gain = 1.0

[fft:cTransformFFT]
reader.dmLevel=winframe
writer.dmLevel=fftc

[fftmagphase:cFFTmagphase]
reader.dmLevel=fftc
writer.dmLevel=fftmag

[mspec:cMelspec]
reader.dmLevel=fftmag
writer.dmLevel=mspec1
htkcompatible = 1
usePower = 1
lofreq = 0
hifreq = 8000

[mfcc:cMfcc]
reader.dmLevel=mspec1
writer.dmLevel=mfcc1
firstMfcc = 1
lastMfcc =  12
htkcompatible = 1

[lpc:cLpc]
reader.dmLevel=framespe
writer.dmLevel=lpc
p=8

[lsp:cLsp]
reader.dmLevel=lpc
writer.dmLevel=lsp

[intens:cIntensity]
reader.dmLevel=frames
writer.dmLevel=intens

[mzcr:cMZcr]
reader.dmLevel=frames
writer.dmLevel=mzcr
zcr=1
amax=0
mcr=0
maxmin=0

[lld:cContourSmoother]
reader.dmLevel=intens;mfcc1;lsp;mzcr;pitch
writer.dmLevel=lld
buffersize=1000
writer.levelconf.isRb=0
writer.levelconf.growDyn=1
; this level must grow to hold ALL the LLD of the full input

// ---- delta regression of LLD ----
[delta1:cDeltaRegression]
reader.dmLevel=lld
writer.dmLevel=lld_de
buffersize=1000
writer.levelconf.isRb=0
writer.levelconf.growDyn=1
deltawin=2
blocksize=1

[energy:cEnergy]
reader.dmLevel=winframe
writer.dmLevel=energy
rms=1
log=0

// the voice activity detection (turn detector)
[turn:cTurnDetector]
reader.dmLevel=energy
writer.dmLevel=isTurn
 ; overwrite data in output level, if it has not been read and level is full
 ; we must set this here, since the level isTurn is a dead-end in our configuration
writer.levelconf.noHang=1
 ; send a message to the functionals component at turn end
messageRecp = functL1
 ; send turn start / turn end event messages to the debug turn wave file output component
eventRecp = turnDump
idx = 0
 ; disable auto adjustment of VAD threshold, this does not work well yet.
autoThreshold = 0
 ; instead, set the threshold manually to a default value.
 ; this derived from the RMS energy from normalised sample values (0..1)
 ; --> !!!!! you might need to adjust this value depending on your recording setup !!!!!!! <-------------------
threshold = 0.0015
 ; --> !!!!! you might need to adjust this value depending on your recording setup !!!!!!! <-------------------


// statistical functionals
[functL1:cFunctionals]
reader.dmLevel=lld;lld_de
writer.dmLevel=func
 ; frameMode = var will enable the functionals component to listen for messages from the turn detector
frameMode = var
functionalsEnabled=Extremes;Regression;Moments;Percentiles
Extremes.maxameandist=0
Extremes.minameandist=0
Extremes.amean=1
Regression.qregc1 = 0
Regression.qregc2 = 0
Regression.qregc3 = 0
Regression.qregerrA = 0
Regression.qregerrQ = 0
Regression.centroid = 0
Regression.linregerrA = 1
Moments.variance = 0
Percentiles.quartiles = 1
Percentiles.iqr = 1

  //////////////////////////////////////////////////////////////////////
 ///////////////////  data output configuration  //////////////////////
//////////////////////////////////////////////////////////////////////

// output recorded and segmented turns as wave files for debugging purposes
// if you do not want this output, then you can disable this component
[turnDump:cWaveSinkCut]
reader.dmLevel = frames

// ----- you can use this to save the features extracted  ------
[arffsink:cArffSink]
reader.dmLevel=func
 ; do not print "frameNumber" attribute to ARFF file
number=0
 ; name of output file as commandline option
filename=\cm[arffout(O){output.arff}:name of WEKA Arff output file]
 ; name of @relation in the ARFF file
relation=\cm[corpus{SMILEfeaturesLive}:corpus name, arff relation]
 ; base name of the current instance, turn number will be appended
instanceBase=liveturn
 ; name of class label
class[0].name = emotion
 ; list of nominal classes OR "numeric"
class[0].type = \cm[classes{unknown}:all classes for arff file attribute]
 ; the class label or value for the current instance
target[0].all = \cm[classlabel{unassigned}:instance class label]
 ; ** NOTE: theoretically the classified class label could be assigned here, however this would require 
 ; ** saving the class label along with the frame in the dataMemory
 ; ** or sending this meta-data via a message (including frame number)
 ; ** neither solution is currently implemented.... :-(
 ;
append=0

// LibSVM dataSink for live classification of extracted features
// continuous arousal from QUB-SAL corpus
[arousal:cLibsvmLiveSink]
reader.dmLevel=func
; model file to use
model=models/emo/sal_aro.emobase.model
; scale file to use
scale=models/emo/sal_aro.emobase.scale
;; you could use the following line to send the classification result to a custom component
 ;resultRecp=emmaSender
 ;resultMessageName=arousal
;; print result to console window
printResult=1

// LibSVM dataSink for live classification of extracted features
// continuous valence from QUB-SAL corpus
[valence:cLibsvmLiveSink]
reader.dmLevel=func
; model file to use
model=models/emo/sal_val.emobase.model
; scale file to use
scale=models/emo/sal_val.emobase.scale
;; you could use the following line to send the classification result to a custom component
 ;resultRecp=emmaSender
 ;resultMessageName=valence
;; print result to console window
printResult=1

// LibSVM dataSink for live classification of extracted features
// Emotion from Berlin Speech Emotion Database (EMO-DB) 
[emodbEmotion:cLibsvmLiveSink]
reader.dmLevel=func
; model file to use
model=models/emo/emodb.emobase.model
; scale file to use
scale=models/emo/emodb.emobase.scale
; class names:
classes=models/emo/emodb.emobase.classes
;; you could use the following line to send the classification result to a custom component
 ;resultRecp=emmaSender
 ;resultMessageName=arousal
;; print result to console window
printResult=1
 ; also display per-class probabilities (of SVM models)
predictProbability=1

// LibSVM dataSink for live classification of extracted features
// Affective state from TUM Airplane Behaviour Corpus (ABC)
[abcAffect:cLibsvmLiveSink]
reader.dmLevel=func
; model file to use
model=models/emo/abc.emobase.model
; scale file to use
scale=models/emo/abc.emobase.scale
; class names:
classes=models/emo/abc.emobase.classes
;; you could use the following line to send the classification result to a custom component
 ;resultRecp=emmaSender
 ;resultMessageName=arousal
;; print result to console window
printResult=1
 ; also display per-class probabilities (of SVM models)
predictProbability=1

// LibSVM dataSink for live classification of extracted features
// Interest level from TUM Audiovisual Interest Corpus (AVIC)
[avicInterest:cLibsvmLiveSink]
reader.dmLevel=func
; model file to use
model=models/emo/avic.emobase.model
; scale file to use
scale=models/emo/avic.emobase.scale
; class names:
classes=models/emo/avic.emobase.classes
;; you could use the following line to send the classification result to a custom component
 ;resultRecp=emmaSender
 ;resultMessageName=arousal
;; print result to console window
printResult=1
 ; also display per-class probabilities (of SVM models)
predictProbability=1



//////---------------------- END -------------------------///////
